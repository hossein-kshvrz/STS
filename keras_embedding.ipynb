{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.077271\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import re\n",
    "from hazm import *\n",
    "\n",
    "FILE_NAME = 'data/fas_newscrawl_2017_100K/fas_newscrawl_2017_100K-sentences.txt'\n",
    "\n",
    "\n",
    "\n",
    "def read_input(input_file):\n",
    "    with open(input_file, encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "        return content\n",
    "\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "raw_content = read_input(FILE_NAME)\n",
    "end = datetime.datetime.now()\n",
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08.335652\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "def my_normalizer(content):\n",
    "    for i in range(len(content)):\n",
    "        sentence = content[i]\n",
    "        # remove number of sentence and \\t\n",
    "        sentence = sentence[sentence.index('\\t')+1:]\n",
    "        # hazm normalizer\n",
    "        normalizer.normalize(sentence)\n",
    "        # my tokenizer\n",
    "        sentence = list(filter(lambda s: s != '', re.compile('[ /\\'\"،؛ء–«»():\\-_.$,\\[\\]!؟\\n\\t]').split(sentence)))\n",
    "        content[i] = sentence\n",
    "#     content = [token for sentence in content for token in sentence]\n",
    "    return content\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "document = my_normalizer(raw_content)\n",
    "end = datetime.datetime.now()\n",
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13', 'دوره', 'بازپرداخت', 'تسهیلات', 'موضوع', 'این', 'تصویب', 'نامه', 'حداکثر', 'یک', 'سال', 'تعیین', 'می', 'گردد']\n"
     ]
    }
   ],
   "source": [
    "print(document[111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107618\n",
      "1618\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "for i in range(len(document)):\n",
    "    vocab.update(document[i])\n",
    "print(len(vocab))\n",
    "print(vocab['دانشگاه'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    document,\n",
    "    size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    sg=0,\n",
    "    workers=10)\n",
    "\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\u202a۸۴', 1.3345073461532593),\n",
       " ('دادگستري', 1.2980746030807495),\n",
       " ('نماينده', 1.2795106172561646),\n",
       " ('اعضاي', 1.276827335357666),\n",
       " ('جلسه\\u200cي', 1.2642039060592651),\n",
       " ('كميته', 1.2526651620864868),\n",
       " ('نمايشگاه', 1.2411503791809082),\n",
       " ('مركز', 1.2402410507202148),\n",
       " ('مطبوعاتي', 1.2330622673034668),\n",
       " ('عواد', 1.2327313423156738)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_cosmul(positive=['تهران', 'فرانسه'], negative=['ایران'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47213\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vectors.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = 'data/text8.zip'\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 1000000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3845, 462], [1636, 296], [3621, 181], [2586, 14], [1713, 1384], [3820, 2670], [3166, 1816], [125, 5], [4116, 6], [6951, 1]] [1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bb270234423d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "similarity = Merge([target, context], mode='cos', dot_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
